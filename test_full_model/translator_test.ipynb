{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461a9946-5a65-4435-94d8-5f538566029f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import eng_to_ipa as ipa\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6642d99e-73d1-41ed-89b8-c11c1c6d16a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class s2s_model:\n",
    "    def __init__(self, max_encoder_len, max_decoder_len, num_encoder_vocab, num_decoder_vocab):\n",
    "        self.latent_dim = 256\n",
    "        self.embedding_dim = 200\n",
    "        self.max_encoder_len = max_encoder_len\n",
    "        self.max_decoder_len = max_decoder_len\n",
    "        self.num_encoder_vocab = num_encoder_vocab\n",
    "        self.num_decoder_vocab = num_decoder_vocab\n",
    "        \n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        \n",
    "        self.training_model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs)\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        self.encoder_inputs = Input(shape=(self.max_encoder_len, ))\n",
    "        self.encoder_embed = Embedding(self.num_encoder_vocab, self.embedding_dim, trainable=True)(self.encoder_inputs)\n",
    "\n",
    "        self.encoder_LSTM1 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout = 0.4, recurrent_dropout = 0.4)\n",
    "        self.encoder_output1, self.state_h1, self.state_c1 = self.encoder_LSTM1(self.encoder_embed)\n",
    "\n",
    "        self.encoder_LSTM2 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        self.encoder_output2, self.state_h2, self.state_c2 = self.encoder_LSTM2(self.encoder_output1) # encoder LSTMs feed into each other\n",
    "\n",
    "        self.encoder_LSTM3 = LSTM(self.latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        self.encoder_output, self.state_h, self.state_c = self.encoder_LSTM3(self.encoder_output2) # final outputs and states to pass to decoder LSTM\n",
    "        \n",
    "    def build_decoder(self):\n",
    "        self.decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "        # define layer architecture, then match to inputs\n",
    "        self.decoder_embed_layer = Embedding(self.num_decoder_vocab, self.embedding_dim, trainable=True)\n",
    "        self.decoder_embed = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        # decoder LSTM layer\n",
    "        self.decoder_LSTM = LSTM(self.latent_dim, return_sequences=True, return_state= True, dropout=0.4, recurrent_dropout=0.2)\n",
    "        self.decoder_outputs, self.decoder_fwd_state, self.decoder_back_state = self.decoder_LSTM(self.decoder_embed, initial_state=[self.state_h, self.state_c])\n",
    "\n",
    "        # dense layer (output layer)\n",
    "        # keras.layers.TimeDistributed layer considers temporal dimension\n",
    "        # Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.\n",
    "        self.decoder_dense = TimeDistributed(Dense(self.num_decoder_vocab, activation='softmax'))\n",
    "        self.decoder_outputs = self.decoder_dense(self.decoder_outputs)\n",
    "        \n",
    "    def compile(self):\n",
    "        self.training_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['acc'])\n",
    "        \n",
    "    def fit(self, x_tr, y_tr_in, y_tr_out, x_test, y_test_in, y_test_out, ep, batch_size):\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "        ck = ModelCheckpoint(filepath='model_best_weights.h5', monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "        Callbacks = [es, ck]\n",
    "        self.training_model.fit([x_tr,y_tr_in], y_tr_out, epochs = ep, callbacks=Callbacks, batch_size = batch_size, validation_data=(([x_test,y_test_in]), y_test_out))\n",
    "    \n",
    "    def build_inference_model(self):\n",
    "        self.inference_encoder_model = Model(inputs= self.encoder_inputs, outputs=[self.encoder_output, self.state_h, self.state_c])\n",
    "\n",
    "        self.inference_encoder_model.save('final_encoder_model.h5')\n",
    "\n",
    "        # decoder setup\n",
    "        self.decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
    "        self.decoder_hidden_state_input = Input(shape=(self.max_encoder_len, self.latent_dim))\n",
    "\n",
    "        self.decoder_embed_i = self.decoder_embed_layer(self.decoder_inputs)\n",
    "\n",
    "        self.decoder_output_i, self.state_h_i, self.state_c_i = self.decoder_LSTM(self.decoder_embed_i, initial_state = [self.decoder_state_input_h, self.decoder_state_input_c])\n",
    "\n",
    "        self.decoder_output_i = self.decoder_dense(self.decoder_output_i)\n",
    "\n",
    "        # final decoder inference model\n",
    "        self.inference_decoder_model = Model([self.decoder_inputs] + [self.decoder_hidden_state_input, self.decoder_state_input_h, self.decoder_state_input_c], [self.decoder_output_i] + [self.state_h_i, self.state_c_i])\n",
    "\n",
    "        # save the final inference model\n",
    "        self.inference_decoder_model.save('final_segmenter_model.h5')\n",
    "        \n",
    "    def decode_sequence(self, input_seq, i2o, o2i):\n",
    "        e_out,e_h, e_c = self.inference_encoder_model.predict(input_seq, verbose = 0)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = o2i['<']\n",
    "\n",
    "        stop_condition = False\n",
    "        decoded_sentence = []\n",
    "\n",
    "        while not stop_condition:\n",
    "            (output_tokens, h, c) = self.inference_decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose = 0)\n",
    "\n",
    "            # Sample a token\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_token = i2o[sampled_token_index]   \n",
    "\n",
    "            if sampled_token != '>':\n",
    "                decoded_sentence += [sampled_token]\n",
    "\n",
    "            # Exit condition: either hit max length or find the stop word.\n",
    "            if (sampled_token == '>') or (len(decoded_sentence) >= self.max_decoder_len):\n",
    "                stop_condition = True\n",
    "\n",
    "            # Update the target sequence (of length 1)\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "            # Update internal states\n",
    "            (e_h, e_c) = (h, c)\n",
    "        return decoded_sentence\n",
    "    def word2seq(self, a2i, input_word):\n",
    "        final_seq = []\n",
    "        for c in input_word:\n",
    "            final_seq += [a2i[c]]\n",
    "        final_seq = pad_sequences([final_seq], maxlen=self.max_encoder_len, padding='post')[0]\n",
    "        return final_seq\n",
    "    \n",
    "    def translate(self, input_word, a2i, i2o, o2i):\n",
    "        seq = self.word2seq(a2i, input_word).reshape(1, self.max_encoder_len)\n",
    "        return self.decode_sequence(seq, i2o, o2i)\n",
    "    def load_inference_model(self, encoder_filename, decoder_filename):\n",
    "        self.inference_encoder_model = load_model(encoder_filename, compile=False)\n",
    "        self.inference_decoder_model = load_model(decoder_filename, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "adaebd41-35c4-4a36-a1a4-0b6ec5d14b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_translator = s2s_model(10, 12, 28, 44)\n",
    "testing_segmenter = s2s_model(9, 12, 27, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1bfb552e-65e0-4b08-9b4b-e5f6413110b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_translator.load_inference_model('final_encoder_model_translator.h5', 'final_decoder_model_translator.h5')\n",
    "testing_segmenter.load_inference_model('final_encoder_model_segmenter.h5', 'final_decoder_model_segmenter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dbfdce3f-f282-4e23-981b-c196bb074f84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipa2i_file = open(\"ipa2i.pkl\",\"rb\")\n",
    "o2i_file = open(\"o2i.pkl\",\"rb\")\n",
    "i2ipa_file = open(\"i2ipa.pkl\",\"rb\")\n",
    "i2o_file = open(\"i2o.pkl\",\"rb\")\n",
    "\n",
    "a2i_file = open(\"a2i.pkl\",\"rb\")\n",
    "o2i_file_seg = open(\"o2i_seg.pkl\",\"rb\")\n",
    "i2a_file = open(\"i2a.pkl\",\"rb\")\n",
    "i2o_file_seg = open(\"i2o_seg.pkl\",\"rb\")\n",
    "\n",
    "new_words_file = open('valid_validation.txt')\n",
    "new_words_list = new_words_file.readlines()\n",
    "new_words_list = [word.strip('\\n') for word in new_words_list]\n",
    "\n",
    "ipa2i = pickle.load(ipa2i_file)\n",
    "o2i = pickle.load(o2i_file)\n",
    "i2ipa = pickle.load(i2ipa_file)\n",
    "i2o = pickle.load(i2o_file)\n",
    "\n",
    "a2i = pickle.load(a2i_file)\n",
    "o2i_seg = pickle.load(o2i_file_seg)\n",
    "i2a = pickle.load(i2a_file)\n",
    "i2o_seg = pickle.load(i2o_file_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b30d98d-2920-4f56-8f89-3b3e8b4b7d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with translations =  496\n",
      "Average  Bleu Score = 0.538431\n"
     ]
    }
   ],
   "source": [
    "output_file = open('output.txt', 'w+', encoding='UTF-8')\n",
    "\n",
    "num_words = 0\n",
    "sum_bleu = 0\n",
    "quit\n",
    "\n",
    "for new_word in new_words_list:\n",
    "    segmented_array = testing_segmenter.translate(new_word, a2i, i2o_seg, o2i_seg)\n",
    "    segmented_string = convert_to_string(segmented_array)\n",
    "    ipa_array = testing_translator.translate(segmented_string, o2i, i2ipa, ipa2i)\n",
    "    translated_string = convert_to_string(ipa_array)\n",
    "    actual_translation = ipa.convert(new_word)\n",
    "    bleu_score = -1.0\n",
    "    if(actual_translation != '*'):\n",
    "        bleu_score = calc_bleu(actual_translation, translated_string)\n",
    "        sum_bleu += bleu_score\n",
    "        num_words += 1\n",
    "    output_file.write(new_word + '\\t' + segmented_string + '\\t' + translated_string + '\\t' + actual_translation + '\\t' + '%f'%bleu_score + '\\n')\n",
    "output_file.write(\"Number of words with translations = \" + str(num_words) + '\\n')\n",
    "output_file.write(\"Average  Bleu Score = \" + '%f'%(sum_bleu/num_words) + '\\n')\n",
    "\n",
    "print(\"Number of words with translations = \", num_words)\n",
    "print(\"Average  Bleu Score = \" + '%f'%(sum_bleu/num_words))\n",
    "\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c768d0-88f6-4820-8dac-899bf6c2ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(array):\n",
    "    final = \"\"\n",
    "    for c in array:\n",
    "        final += c\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3d4e0c1-a185-4686-97a0-e0b0aec8f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bleu(ref, cand):\n",
    "    reference = [*ref]\n",
    "    candidate = [*cand]\n",
    "    return sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "164d55cc-86d3-4c7b-be90-7c35c761367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This program does its best to segment a given mono syllabic word into its constituent onset-rime pair, and outputs an IPA transcription of the word.\n",
      "This network works best on single syllable words with common rimes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a monosyllabic word that does not contain the letter X, or 'quit' to exit:  trop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmented by onset-rime pair: tr,op\n",
      "Best guess ipa translation: trop*\n",
      "Actual ipa translation: trop*\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a monosyllabic word that does not contain the letter X, or 'quit' to exit:  mlop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmented by onset-rime pair: m,op\n",
      "Best guess ipa translation: m…ëp\n",
      "Actual ipa translation: mlop*\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a monosyllabic word that does not contain the letter X, or 'quit' to exit:  krhing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmented by onset-rime pair: kh,ump \n",
      "Best guess ipa translation: khump*\n",
      "Actual ipa translation: krhing*\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a monosyllabic word that does not contain the letter X, or 'quit' to exit:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "exit_words = ['quit', 'exit', 'stop']\n",
    "\n",
    "input_word = \"\"\n",
    "\n",
    "print(\"This program does its best to segment a given mono syllabic word into its constituent onset-rime pair, and outputs an IPA transcription of the word.\")\n",
    "print(\"This network works best on single syllable words with common rimes.\")\n",
    "\n",
    "while(True):\n",
    "    input_word = input(\"Enter a monosyllabic word that does not contain the letter X, or 'quit' to exit: \")\n",
    "    if(input_word in exit_words):\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    segmented_array = testing_segmenter.translate(input_word, a2i, i2o_seg, o2i_seg)\n",
    "    segmented_string = convert_to_string(segmented_array)\n",
    "    ipa_array = testing_translator.translate(segmented_string, o2i, i2ipa, ipa2i)\n",
    "    translated_string = convert_to_string(ipa_array)\n",
    "    actual_translation = ipa.convert(input_word)\n",
    "    print(\"Word segmented by onset-rime pair: \" + segmented_string)\n",
    "    print(\"Best guess ipa translation: \" + translated_string)\n",
    "    print(\"Actual ipa translation: \" + actual_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f5176-3cbf-41c9-928a-251b03772e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:laptop_sketchbook] *",
   "language": "python",
   "name": "conda-env-laptop_sketchbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
